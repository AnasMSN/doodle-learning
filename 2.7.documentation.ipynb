{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f6ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022d957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__text_signature__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.arange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709c742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.distributions in torch:\n",
      "\n",
      "NAME\n",
      "    torch.distributions\n",
      "\n",
      "DESCRIPTION\n",
      "    The ``distributions`` package contains parameterizable probability distributions\n",
      "    and sampling functions. This allows the construction of stochastic computation\n",
      "    graphs and stochastic gradient estimators for optimization. This package\n",
      "    generally follows the design of the `TensorFlow Distributions`_ package.\n",
      "    \n",
      "    .. _`TensorFlow Distributions`:\n",
      "        https://arxiv.org/abs/1711.10604\n",
      "    \n",
      "    It is not possible to directly backpropagate through random samples. However,\n",
      "    there are two main methods for creating surrogate functions that can be\n",
      "    backpropagated through. These are the score function estimator/likelihood ratio\n",
      "    estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly\n",
      "    seen as the basis for policy gradient methods in reinforcement learning, and the\n",
      "    pathwise derivative estimator is commonly seen in the reparameterization trick\n",
      "    in variational autoencoders. Whilst the score function only requires the value\n",
      "    of samples :math:`f(x)`, the pathwise derivative requires the derivative\n",
      "    :math:`f'(x)`. The next sections discuss these two in a reinforcement learning\n",
      "    example. For more details see\n",
      "    `Gradient Estimation Using Stochastic Computation Graphs`_ .\n",
      "    \n",
      "    .. _`Gradient Estimation Using Stochastic Computation Graphs`:\n",
      "         https://arxiv.org/abs/1506.05254\n",
      "    \n",
      "    Score function\n",
      "    ^^^^^^^^^^^^^^\n",
      "    \n",
      "    When the probability density function is differentiable with respect to its\n",
      "    parameters, we only need :meth:`~torch.distributions.Distribution.sample` and\n",
      "    :meth:`~torch.distributions.Distribution.log_prob` to implement REINFORCE:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\Delta\\theta  = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta}\n",
      "    \n",
      "    where :math:`\\theta` are the parameters, :math:`\\alpha` is the learning rate,\n",
      "    :math:`r` is the reward and :math:`p(a|\\pi^\\theta(s))` is the probability of\n",
      "    taking action :math:`a` in state :math:`s` given policy :math:`\\pi^\\theta`.\n",
      "    \n",
      "    In practice we would sample an action from the output of a network, apply this\n",
      "    action in an environment, and then use ``log_prob`` to construct an equivalent\n",
      "    loss function. Note that we use a negative because optimizers use gradient\n",
      "    descent, whilst the rule above assumes gradient ascent. With a categorical\n",
      "    policy, the code for implementing REINFORCE would be as follows::\n",
      "    \n",
      "        probs = policy_network(state)\n",
      "        # Note that this is equivalent to what used to be called multinomial\n",
      "        m = Categorical(probs)\n",
      "        action = m.sample()\n",
      "        next_state, reward = env.step(action)\n",
      "        loss = -m.log_prob(action) * reward\n",
      "        loss.backward()\n",
      "    \n",
      "    Pathwise derivative\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    The other way to implement these stochastic/policy gradients would be to use the\n",
      "    reparameterization trick from the\n",
      "    :meth:`~torch.distributions.Distribution.rsample` method, where the\n",
      "    parameterized random variable can be constructed via a parameterized\n",
      "    deterministic function of a parameter-free random variable. The reparameterized\n",
      "    sample therefore becomes differentiable. The code for implementing the pathwise\n",
      "    derivative would be as follows::\n",
      "    \n",
      "        params = policy_network(state)\n",
      "        m = Normal(*params)\n",
      "        # Any distribution with .has_rsample == True could work based on the application\n",
      "        action = m.rsample()\n",
      "        next_state, reward = env.step(action)  # Assuming that reward is differentiable\n",
      "        loss = -reward\n",
      "        loss.backward()\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    bernoulli\n",
      "    beta\n",
      "    binomial\n",
      "    categorical\n",
      "    cauchy\n",
      "    chi2\n",
      "    constraint_registry\n",
      "    constraints\n",
      "    continuous_bernoulli\n",
      "    dirichlet\n",
      "    distribution\n",
      "    exp_family\n",
      "    exponential\n",
      "    fishersnedecor\n",
      "    gamma\n",
      "    geometric\n",
      "    gumbel\n",
      "    half_cauchy\n",
      "    half_normal\n",
      "    independent\n",
      "    inverse_gamma\n",
      "    kl\n",
      "    kumaraswamy\n",
      "    laplace\n",
      "    lkj_cholesky\n",
      "    log_normal\n",
      "    logistic_normal\n",
      "    lowrank_multivariate_normal\n",
      "    mixture_same_family\n",
      "    multinomial\n",
      "    multivariate_normal\n",
      "    negative_binomial\n",
      "    normal\n",
      "    one_hot_categorical\n",
      "    pareto\n",
      "    poisson\n",
      "    relaxed_bernoulli\n",
      "    relaxed_categorical\n",
      "    studentT\n",
      "    transformed_distribution\n",
      "    transforms\n",
      "    uniform\n",
      "    utils\n",
      "    von_mises\n",
      "    weibull\n",
      "    wishart\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        torch.distributions.distribution.Distribution\n",
      "            torch.distributions.binomial.Binomial\n",
      "            torch.distributions.categorical.Categorical\n",
      "            torch.distributions.cauchy.Cauchy\n",
      "            torch.distributions.exp_family.ExponentialFamily\n",
      "                torch.distributions.bernoulli.Bernoulli\n",
      "                torch.distributions.beta.Beta\n",
      "                torch.distributions.continuous_bernoulli.ContinuousBernoulli\n",
      "                torch.distributions.dirichlet.Dirichlet\n",
      "                torch.distributions.exponential.Exponential\n",
      "                torch.distributions.gamma.Gamma\n",
      "                    torch.distributions.chi2.Chi2\n",
      "                torch.distributions.normal.Normal\n",
      "                torch.distributions.poisson.Poisson\n",
      "                torch.distributions.wishart.Wishart\n",
      "            torch.distributions.fishersnedecor.FisherSnedecor\n",
      "            torch.distributions.geometric.Geometric\n",
      "            torch.distributions.independent.Independent\n",
      "            torch.distributions.laplace.Laplace\n",
      "            torch.distributions.lkj_cholesky.LKJCholesky\n",
      "            torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal\n",
      "            torch.distributions.mixture_same_family.MixtureSameFamily\n",
      "            torch.distributions.multinomial.Multinomial\n",
      "            torch.distributions.multivariate_normal.MultivariateNormal\n",
      "            torch.distributions.negative_binomial.NegativeBinomial\n",
      "            torch.distributions.one_hot_categorical.OneHotCategorical\n",
      "                torch.distributions.one_hot_categorical.OneHotCategoricalStraightThrough\n",
      "            torch.distributions.studentT.StudentT\n",
      "            torch.distributions.transformed_distribution.TransformedDistribution\n",
      "                torch.distributions.gumbel.Gumbel\n",
      "                torch.distributions.half_cauchy.HalfCauchy\n",
      "                torch.distributions.half_normal.HalfNormal\n",
      "                torch.distributions.inverse_gamma.InverseGamma\n",
      "                torch.distributions.kumaraswamy.Kumaraswamy\n",
      "                torch.distributions.log_normal.LogNormal\n",
      "                torch.distributions.logistic_normal.LogisticNormal\n",
      "                torch.distributions.pareto.Pareto\n",
      "                torch.distributions.relaxed_bernoulli.RelaxedBernoulli\n",
      "                torch.distributions.relaxed_categorical.RelaxedOneHotCategorical\n",
      "                torch.distributions.weibull.Weibull\n",
      "            torch.distributions.uniform.Uniform\n",
      "            torch.distributions.von_mises.VonMises\n",
      "        torch.distributions.transforms.Transform\n",
      "            torch.distributions.transforms.AbsTransform\n",
      "            torch.distributions.transforms.AffineTransform\n",
      "            torch.distributions.transforms.CatTransform\n",
      "            torch.distributions.transforms.ComposeTransform\n",
      "            torch.distributions.transforms.CorrCholeskyTransform\n",
      "            torch.distributions.transforms.CumulativeDistributionTransform\n",
      "            torch.distributions.transforms.ExpTransform\n",
      "            torch.distributions.transforms.IndependentTransform\n",
      "            torch.distributions.transforms.LowerCholeskyTransform\n",
      "            torch.distributions.transforms.PositiveDefiniteTransform\n",
      "            torch.distributions.transforms.PowerTransform\n",
      "            torch.distributions.transforms.ReshapeTransform\n",
      "            torch.distributions.transforms.SigmoidTransform\n",
      "            torch.distributions.transforms.SoftmaxTransform\n",
      "            torch.distributions.transforms.SoftplusTransform\n",
      "            torch.distributions.transforms.StackTransform\n",
      "            torch.distributions.transforms.StickBreakingTransform\n",
      "            torch.distributions.transforms.TanhTransform\n",
      "    \n",
      "    class AbsTransform(Transform)\n",
      "     |  AbsTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = |x|`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AbsTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class AffineTransform(Transform)\n",
      "     |  AffineTransform(loc, scale, event_dim=0, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the pointwise affine mapping :math:`y = \\text{loc} + \\text{scale} \\times x`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor or float): Location parameter.\n",
      "     |      scale (Tensor or float): Scale parameter.\n",
      "     |      event_dim (int): Optional size of `event_shape`. This should be zero\n",
      "     |          for univariate random variables, 1 for distributions over vectors,\n",
      "     |          2 for distributions over matrices, etc.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AffineTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, event_dim=0, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class Bernoulli(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Bernoulli(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Bernoulli distribution parameterized by :attr:`probs`\n",
      "     |  or :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  Samples are binary (0 or 1). They take the value `1` with probability `p`\n",
      "     |  and `0` with probability `1 - p`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Bernoulli(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()  # 30% chance 1; 70% chance 0\n",
      "     |      tensor([ 0.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Bernoulli\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = Boolean()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Beta(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Beta(concentration1, concentration0, validate_args=None)\n",
      "     |  \n",
      "     |  Beta distribution parameterized by :attr:`concentration1` and :attr:`concentration0`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n",
      "     |      >>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration1 (float or Tensor): 1st concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      concentration0 (float or Tensor): 2nd concentration parameter of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Beta\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration1, concentration0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = ()) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  concentration0\n",
      "     |  \n",
      "     |  concentration1\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'co...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Binomial(torch.distributions.distribution.Distribution)\n",
      "     |  Binomial(total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Binomial distribution parameterized by :attr:`total_count` and\n",
      "     |  either :attr:`probs` or :attr:`logits` (but not both). :attr:`total_count` must be\n",
      "     |  broadcastable with :attr:`probs`/:attr:`logits`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n",
      "     |      >>> x = m.sample()\n",
      "     |      tensor([   0.,   22.,   71.,  100.])\n",
      "     |  \n",
      "     |      >>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n",
      "     |      >>> x = m.sample()\n",
      "     |      tensor([[ 4.,  5.],\n",
      "     |              [ 7.,  6.]])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (int or Tensor): number of Bernoulli trials\n",
      "     |      probs (Tensor): Event probabilities\n",
      "     |      logits (Tensor): Event log-odds\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Binomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class CatTransform(Transform)\n",
      "     |  CatTransform(tseq, dim=0, lengths=None, cache_size=0)\n",
      "     |  \n",
      "     |  Transform functor that applies a sequence of transforms `tseq`\n",
      "     |  component-wise to each submatrix at `dim`, of length `lengths[dim]`,\n",
      "     |  in a way compatible with :func:`torch.cat`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |     x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\n",
      "     |     x = torch.cat([x0, x0], dim=0)\n",
      "     |     t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\n",
      "     |     t = CatTransform([t0, t0], dim=0, lengths=[20, 20])\n",
      "     |     y = t(x)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CatTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tseq, dim=0, lengths=None, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  length\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'transforms': typing.List[torch.distributions.trans...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Categorical(torch.distributions.distribution.Distribution)\n",
      "     |  Categorical(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a categorical distribution parameterized by either :attr:`probs` or\n",
      "     |  :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      It is equivalent to the distribution that :func:`torch.multinomial`\n",
      "     |      samples from.\n",
      "     |  \n",
      "     |  Samples are integers from :math:`\\{0, \\ldots, K-1\\}` where `K` is ``probs.size(-1)``.\n",
      "     |  \n",
      "     |  If `probs` is 1-dimensional with length-`K`, each element is the relative probability\n",
      "     |  of sampling the class at that index.\n",
      "     |  \n",
      "     |  If `probs` is N-dimensional, the first N-1 dimensions are treated as a batch of\n",
      "     |  relative probability vectors.\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  See also: :func:`torch.multinomial`\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
      "     |      >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor(3)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Categorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Cauchy(torch.distributions.distribution.Distribution)\n",
      "     |  Cauchy(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\n",
      "     |  independent normally distributed random variables with means `0` follows a\n",
      "     |  Cauchy distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\n",
      "     |      tensor([ 2.3214])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mode or median of the distribution.\n",
      "     |      scale (float or Tensor): half width at half maximum.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Cauchy\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Chi2(torch.distributions.gamma.Gamma)\n",
      "     |  Chi2(df, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Chi-squared distribution parameterized by shape parameter :attr:`df`.\n",
      "     |  This is exactly equivalent to ``Gamma(alpha=0.5*df, beta=0.5)``\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Chi2(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Chi2 distributed with shape df=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df (float or Tensor): shape parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Chi2\n",
      "     |      torch.distributions.gamma.Gamma\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  df\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'df': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ComposeTransform(Transform)\n",
      "     |  ComposeTransform(parts: List[torch.distributions.transforms.Transform], cache_size=0)\n",
      "     |  \n",
      "     |  Composes multiple transforms in a chain.\n",
      "     |  The transforms being composed are responsible for caching.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      parts (list of :class:`Transform`): A list of transforms to compose.\n",
      "     |      cache_size (int): Size of cache. If zero, no caching is done. If one,\n",
      "     |          the latest single value is cached. Only 0 and 1 are supported.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ComposeTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, parts: List[torch.distributions.transforms.Transform], cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class ContinuousBernoulli(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\n",
      "     |  \n",
      "     |  Creates a continuous Bernoulli distribution parameterized by :attr:`probs`\n",
      "     |  or :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  The distribution is supported in [0, 1] and parameterized by 'probs' (in\n",
      "     |  (0,1)) or 'logits' (real-valued). Note that, unlike the Bernoulli, 'probs'\n",
      "     |  does not correspond to a probability and 'logits' does not correspond to\n",
      "     |  log-odds, but the same names are used due to the similarity with the\n",
      "     |  Bernoulli. See [1] for more details.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.2538])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): (0,1) valued parameters\n",
      "     |      logits (Number, Tensor): real valued parameters whose sigmoid matches 'probs'\n",
      "     |  \n",
      "     |  [1] The continuous Bernoulli: fixing a pervasive error in variational\n",
      "     |  autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\n",
      "     |  https://arxiv.org/abs/1907.06845\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ContinuousBernoulli\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class CorrCholeskyTransform(Transform)\n",
      "     |  CorrCholeskyTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transforms an uncontrained real vector :math:`x` with length :math:`D*(D-1)/2` into the\n",
      "     |  Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\n",
      "     |  triangular matrix with positive diagonals and unit Euclidean norm for each row.\n",
      "     |  The transform is processed as follows:\n",
      "     |  \n",
      "     |      1. First we convert x into a lower triangular matrix in row order.\n",
      "     |      2. For each row :math:`X_i` of the lower triangular part, we apply a *signed* version of\n",
      "     |         class :class:`StickBreakingTransform` to transform :math:`X_i` into a\n",
      "     |         unit Euclidean length vector using the following steps:\n",
      "     |         - Scales into the interval :math:`(-1, 1)` domain: :math:`r_i = \\tanh(X_i)`.\n",
      "     |         - Transforms into an unsigned domain: :math:`z_i = r_i^2`.\n",
      "     |         - Applies :math:`s_i = StickBreakingTransform(z_i)`.\n",
      "     |         - Transforms back into signed domain: :math:`y_i = sign(r_i) * \\sqrt{s_i}`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CorrCholeskyTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y, intermediates=None)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = CorrCholesky()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class CumulativeDistributionTransform(Transform)\n",
      "     |  CumulativeDistributionTransform(distribution, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the cumulative distribution function of a probability distribution.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      distribution (Distribution): Distribution whose cumulative distribution function to use for\n",
      "     |          the transformation.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      # Construct a Gaussian copula from a multivariate normal.\n",
      "     |      base_dist = MultivariateNormal(\n",
      "     |          loc=torch.zeros(2),\n",
      "     |          scale_tril=LKJCholesky(2).sample(),\n",
      "     |      )\n",
      "     |      transform = CumulativeDistributionTransform(Normal(0, 1))\n",
      "     |      copula = TransformedDistribution(base_dist, [transform])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CumulativeDistributionTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, distribution, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Dirichlet(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Dirichlet(concentration, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Dirichlet distribution parameterized by concentration :attr:`concentration`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n",
      "     |      >>> m.sample()  # Dirichlet distributed with concentration [0.5, 0.5]\n",
      "     |      tensor([ 0.1046,  0.8954])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration (Tensor): concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Dirichlet\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = ()) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': IndependentConstraint(GreaterThan(...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Distribution(builtins.object)\n",
      "     |  Distribution(batch_shape: torch.Size = torch.Size([]), event_shape: torch.Size = torch.Size([]), validate_args: Optional[bool] = None)\n",
      "     |  \n",
      "     |  Distribution is the abstract base class for probability distributions.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, batch_shape: torch.Size = torch.Size([]), event_shape: torch.Size = torch.Size([]), validate_args: Optional[bool] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape: Union[torch.Size, List[int], Tuple[int, ...]], _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  arg_constraints\n",
      "     |      Returns a dictionary from argument names to\n",
      "     |      :class:`~torch.distributions.constraints.Constraint` objects that\n",
      "     |      should be satisfied by each argument of this distribution. Args that\n",
      "     |      are not tensors need not appear in this dict.\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class ExpTransform(Transform)\n",
      "     |  ExpTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\exp(x)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExpTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class Exponential(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Exponential(rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Exponential distribution parameterized by :attr:`rate`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Exponential(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Exponential distributed with rate=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      rate (float or Tensor): rate = 1 / scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Exponential\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ExponentialFamily(torch.distributions.distribution.Distribution)\n",
      "     |  ExponentialFamily(batch_shape: torch.Size = torch.Size([]), event_shape: torch.Size = torch.Size([]), validate_args: Optional[bool] = None)\n",
      "     |  \n",
      "     |  ExponentialFamily is the abstract base class for probability distributions belonging to an\n",
      "     |  exponential family, whose probability mass/density function has the form is defined below\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x))\n",
      "     |  \n",
      "     |  where :math:`\\theta` denotes the natural parameters, :math:`t(x)` denotes the sufficient statistic,\n",
      "     |  :math:`F(\\theta)` is the log normalizer function for a given family and :math:`k(x)` is the carrier\n",
      "     |  measure.\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      This class is an intermediary between the `Distribution` class and distributions which belong\n",
      "     |      to an exponential family mainly to check the correctness of the `.entropy()` and analytic KL\n",
      "     |      divergence methods. We use this class to compute the entropy and KL divergence using the AD\n",
      "     |      framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and\n",
      "     |      Cross-entropies of Exponential Families).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __init__(self, batch_shape: torch.Size = torch.Size([]), event_shape: torch.Size = torch.Size([]), validate_args: Optional[bool] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape: Union[torch.Size, List[int], Tuple[int, ...]], _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  arg_constraints\n",
      "     |      Returns a dictionary from argument names to\n",
      "     |      :class:`~torch.distributions.constraints.Constraint` objects that\n",
      "     |      should be satisfied by each argument of this distribution. Args that\n",
      "     |      are not tensors need not appear in this dict.\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class FisherSnedecor(torch.distributions.distribution.Distribution)\n",
      "     |  FisherSnedecor(df1, df2, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Fisher-Snedecor distribution parameterized by :attr:`df1` and :attr:`df2`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\n",
      "     |      tensor([ 0.2453])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df1 (float or Tensor): degrees of freedom parameter 1\n",
      "     |      df2 (float or Tensor): degrees of freedom parameter 2\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FisherSnedecor\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df1, df2, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': Greater...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Gamma(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Gamma(concentration, rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Gamma distribution parameterized by shape :attr:`concentration` and :attr:`rate`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Gamma distributed with concentration=1 and rate=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration (float or Tensor): shape parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      rate (float or Tensor): rate = 1 / scale of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Gamma\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rat...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Geometric(torch.distributions.distribution.Distribution)\n",
      "     |  Geometric(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Geometric distribution parameterized by :attr:`probs`,\n",
      "     |  where :attr:`probs` is the probability of success of Bernoulli trials.\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      P(X=k) = (1-p)^{k} p, k = 0, 1, ...\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      :func:`torch.distributions.geometric.Geometric` :math:`(k+1)`-th trial is the first success\n",
      "     |      hence draws samples in :math:`\\{0, 1, \\ldots\\}`, whereas\n",
      "     |      :func:`torch.Tensor.geometric_` `k`-th trial is the first success hence draws samples in :math:`\\{1, 2, \\ldots\\}`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Geometric(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\n",
      "     |      tensor([ 2.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`. Must be in range (0, 1]\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Geometric\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Gumbel(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Gumbel(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Gumbel Distribution.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\n",
      "     |      tensor([ 1.0124])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): Location parameter of the distribution\n",
      "     |      scale (float or Tensor): Scale parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Gumbel\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class HalfCauchy(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  HalfCauchy(scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a half-Cauchy distribution parameterized by `scale` where::\n",
      "     |  \n",
      "     |      X ~ Cauchy(0, scale)\n",
      "     |      Y = |X| ~ HalfCauchy(scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = HalfCauchy(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # half-cauchy distributed with scale=1\n",
      "     |      tensor([ 2.3214])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): scale of the full Cauchy distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HalfCauchy\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, prob)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class HalfNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  HalfNormal(scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a half-normal distribution parameterized by `scale` where::\n",
      "     |  \n",
      "     |      X ~ Normal(0, scale)\n",
      "     |      Y = |X| ~ HalfNormal(scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = HalfNormal(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # half-normal distributed with scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): scale of the full Normal distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HalfNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, prob)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Independent(torch.distributions.distribution.Distribution)\n",
      "     |  Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)\n",
      "     |  \n",
      "     |  Reinterprets some of the batch dims of a distribution as event dims.\n",
      "     |  \n",
      "     |  This is mainly useful for changing the shape of the result of\n",
      "     |  :meth:`log_prob`. For example to create a diagonal Normal distribution with\n",
      "     |  the same shape as a Multivariate Normal distribution (so they are\n",
      "     |  interchangeable), you can::\n",
      "     |  \n",
      "     |      >>> from torch.distributions.multivariate_normal import MultivariateNormal\n",
      "     |      >>> from torch.distributions.normal import Normal\n",
      "     |      >>> loc = torch.zeros(3)\n",
      "     |      >>> scale = torch.ones(3)\n",
      "     |      >>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n",
      "     |      >>> [mvn.batch_shape, mvn.event_shape]\n",
      "     |      [torch.Size([]), torch.Size([3])]\n",
      "     |      >>> normal = Normal(loc, scale)\n",
      "     |      >>> [normal.batch_shape, normal.event_shape]\n",
      "     |      [torch.Size([3]), torch.Size([])]\n",
      "     |      >>> diagn = Independent(normal, 1)\n",
      "     |      >>> [diagn.batch_shape, diagn.event_shape]\n",
      "     |      [torch.Size([]), torch.Size([3])]\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      base_distribution (torch.distributions.distribution.Distribution): a\n",
      "     |          base distribution\n",
      "     |      reinterpreted_batch_ndims (int): the number of batch dims to\n",
      "     |          reinterpret as event dims\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Independent\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_distribution, reinterpreted_batch_ndims, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  has_enumerate_support\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class IndependentTransform(Transform)\n",
      "     |  IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)\n",
      "     |  \n",
      "     |  Wrapper around another transform to treat\n",
      "     |  ``reinterpreted_batch_ndims``-many extra of the right most dimensions as\n",
      "     |  dependent. This has no effect on the forward or backward transforms, but\n",
      "     |  does sum out ``reinterpreted_batch_ndims``-many of the rightmost dimensions\n",
      "     |  in :meth:`log_abs_det_jacobian`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      base_transform (:class:`Transform`): A base transform.\n",
      "     |      reinterpreted_batch_ndims (int): The number of extra rightmost\n",
      "     |          dimensions to treat as dependent.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndependentTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_transform, reinterpreted_batch_ndims, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class InverseGamma(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  InverseGamma(concentration, rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates an inverse gamma distribution parameterized by :attr:`concentration` and :attr:`rate`\n",
      "     |  where::\n",
      "     |  \n",
      "     |      X ~ Gamma(concentration, rate)\n",
      "     |      Y = 1 / X ~ InverseGamma(concentration, rate)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterinistic\")\n",
      "     |      >>> m = InverseGamma(torch.tensor([2.0]), torch.tensor([3.0]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 1.2953])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration (float or Tensor): shape parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      rate (float or Tensor): rate = 1 / scale of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      InverseGamma\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  concentration\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  rate\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rat...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Kumaraswamy(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Kumaraswamy(concentration1, concentration0, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Kumaraswamy distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\n",
      "     |      tensor([ 0.1729])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration1 (float or Tensor): 1st concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      concentration0 (float or Tensor): 2nd concentration parameter of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Kumaraswamy\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration1, concentration0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'co...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LKJCholesky(torch.distributions.distribution.Distribution)\n",
      "     |  LKJCholesky(dim, concentration=1.0, validate_args=None)\n",
      "     |  \n",
      "     |  LKJ distribution for lower Cholesky factor of correlation matrices.\n",
      "     |  The distribution is controlled by ``concentration`` parameter :math:`\\eta`\n",
      "     |  to make the probability of the correlation matrix :math:`M` generated from\n",
      "     |  a Cholesky factor proportional to :math:`\\det(M)^{\\eta - 1}`. Because of that,\n",
      "     |  when ``concentration == 1``, we have a uniform distribution over Cholesky\n",
      "     |  factors of correlation matrices::\n",
      "     |  \n",
      "     |      L ~ LKJCholesky(dim, concentration)\n",
      "     |      X = L @ L' ~ LKJCorr(dim, concentration)\n",
      "     |  \n",
      "     |  Note that this distribution samples the\n",
      "     |  Cholesky factor of correlation matrices and not the correlation matrices\n",
      "     |  themselves and thereby differs slightly from the derivations in [1] for\n",
      "     |  the `LKJCorr` distribution. For sampling, this uses the Onion method from\n",
      "     |  [1] Section 3.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> l = LKJCholesky(3, 0.5)\n",
      "     |      >>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\n",
      "     |      tensor([[ 1.0000,  0.0000,  0.0000],\n",
      "     |              [ 0.3516,  0.9361,  0.0000],\n",
      "     |              [-0.1899,  0.4748,  0.8593]])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      dimension (dim): dimension of the matrices\n",
      "     |      concentration (float or Tensor): concentration/shape parameter of the\n",
      "     |          distribution (often referred to as eta)\n",
      "     |  \n",
      "     |  **References**\n",
      "     |  \n",
      "     |  [1] `Generating random correlation matrices based on vines and extended onion method` (2009),\n",
      "     |  Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n",
      "     |  Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LKJCholesky\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dim, concentration=1.0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  support = CorrCholesky()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Laplace(torch.distributions.distribution.Distribution)\n",
      "     |  Laplace(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Laplace distribution parameterized by :attr:`loc` and :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Laplace distributed with loc=0, scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the distribution\n",
      "     |      scale (float or Tensor): scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Laplace\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LogNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  LogNormal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a log-normal distribution parameterized by\n",
      "     |  :attr:`loc` and :attr:`scale` where::\n",
      "     |  \n",
      "     |      X ~ Normal(loc, scale)\n",
      "     |      Y = exp(X) ~ LogNormal(loc, scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # log-normal distributed with mean=0 and stddev=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of log of distribution\n",
      "     |      scale (float or Tensor): standard deviation of log of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  loc\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LogisticNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  LogisticNormal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a logistic-normal distribution parameterized by :attr:`loc` and :attr:`scale`\n",
      "     |  that define the base `Normal` distribution transformed with the\n",
      "     |  `StickBreakingTransform` such that::\n",
      "     |  \n",
      "     |      X ~ LogisticNormal(loc, scale)\n",
      "     |      Y = log(X / (1 - X.cumsum(-1)))[..., :-1] ~ Normal(loc, scale)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the base distribution\n",
      "     |      scale (float or Tensor): standard deviation of the base distribution\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # logistic-normal distributed with mean=(0, 0, 0) and stddev=(1, 1, 1)\n",
      "     |      >>> # of the base Normal distribution\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = LogisticNormal(torch.tensor([0.0] * 3), torch.tensor([1.0] * 3))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.7653,  0.0341,  0.0579,  0.1427])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  loc\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LowRankMultivariateNormal(torch.distributions.distribution.Distribution)\n",
      "     |  LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a multivariate normal distribution with covariance matrix having a low-rank form\n",
      "     |  parameterized by :attr:`cov_factor` and :attr:`cov_diag`::\n",
      "     |  \n",
      "     |      covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n",
      "     |      >>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\n",
      "     |      tensor([-0.2102, -0.5429])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor): mean of the distribution with shape `batch_shape + event_shape`\n",
      "     |      cov_factor (Tensor): factor part of low-rank form of covariance matrix with shape\n",
      "     |          `batch_shape + event_shape + (rank,)`\n",
      "     |      cov_diag (Tensor): diagonal part of low-rank form of covariance matrix with shape\n",
      "     |          `batch_shape + event_shape`\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      The computation for determinant and inverse of covariance matrix is avoided when\n",
      "     |      `cov_factor.shape[1] << cov_factor.shape[0]` thanks to `Woodbury matrix identity\n",
      "     |      <https://en.wikipedia.org/wiki/Woodbury_matrix_identity>`_ and\n",
      "     |      `matrix determinant lemma <https://en.wikipedia.org/wiki/Matrix_determinant_lemma>`_.\n",
      "     |      Thanks to these formulas, we just need to compute the determinant and inverse of\n",
      "     |      the small size \"capacitance\" matrix::\n",
      "     |  \n",
      "     |          capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LowRankMultivariateNormal\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, cov_factor, cov_diag, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LowerCholeskyTransform(Transform)\n",
      "     |  LowerCholeskyTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained matrices to lower-triangular matrices with\n",
      "     |  nonnegative diagonal entries.\n",
      "     |  \n",
      "     |  This is useful for parameterizing positive definite matrices in terms of\n",
      "     |  their Cholesky factorization.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LowerCholeskyTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = LowerCholesky()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 2)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class MixtureSameFamily(torch.distributions.distribution.Distribution)\n",
      "     |  MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)\n",
      "     |  \n",
      "     |  The `MixtureSameFamily` distribution implements a (batch of) mixture\n",
      "     |  distribution where all component are from different parameterizations of\n",
      "     |  the same distribution type. It is parameterized by a `Categorical`\n",
      "     |  \"selecting distribution\" (over `k` component) and a component\n",
      "     |  distribution, i.e., a `Distribution` with a rightmost batch shape\n",
      "     |  (equal to `[k]`) which indexes each (batch of) component.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |      >>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n",
      "     |      >>> # weighted normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.ones(5,))\n",
      "     |      >>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |      >>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally\n",
      "     |      >>> # weighted bivariate normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.ones(5,))\n",
      "     |      >>> comp = D.Independent(D.Normal(\n",
      "     |      ...          torch.randn(5,2), torch.rand(5,2)), 1)\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |      >>> # Construct a batch of 3 Gaussian Mixture Models in 2D each\n",
      "     |      >>> # consisting of 5 random weighted bivariate normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.rand(3,5))\n",
      "     |      >>> comp = D.Independent(D.Normal(\n",
      "     |      ...         torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      mixture_distribution: `torch.distributions.Categorical`-like\n",
      "     |          instance. Manages the probability of selecting component.\n",
      "     |          The number of categories must match the rightmost batch\n",
      "     |          dimension of the `component_distribution`. Must have either\n",
      "     |          scalar `batch_shape` or `batch_shape` matching\n",
      "     |          `component_distribution.batch_shape[:-1]`\n",
      "     |      component_distribution: `torch.distributions.Distribution`-like\n",
      "     |          instance. Right-most batch dimension indexes component.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MixtureSameFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, mixture_distribution, component_distribution, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, x)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, x)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  component_distribution\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mixture_distribution\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Multinomial(torch.distributions.distribution.Distribution)\n",
      "     |  Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Multinomial distribution parameterized by :attr:`total_count` and\n",
      "     |  either :attr:`probs` or :attr:`logits` (but not both). The innermost dimension of\n",
      "     |  :attr:`probs` indexes over categories. All other dimensions index over batches.\n",
      "     |  \n",
      "     |  Note that :attr:`total_count` need not be specified if only :meth:`log_prob` is\n",
      "     |  called (see example below)\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  -   :meth:`sample` requires a single shared `total_count` for all\n",
      "     |      parameters and samples.\n",
      "     |  -   :meth:`log_prob` allows different `total_count` for each parameter and\n",
      "     |      sample.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +SKIP(\"FIXME: found invalid values\")\n",
      "     |      >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n",
      "     |      >>> x = m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor([ 21.,  24.,  30.,  25.])\n",
      "     |  \n",
      "     |      >>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\n",
      "     |      tensor([-4.1338])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (int): number of trials\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Multinomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'total_count': <class 'int'>}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class MultivariateNormal(torch.distributions.distribution.Distribution)\n",
      "     |  MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a multivariate normal (also called Gaussian) distribution\n",
      "     |  parameterized by a mean vector and a covariance matrix.\n",
      "     |  \n",
      "     |  The multivariate normal distribution can be parameterized either\n",
      "     |  in terms of a positive definite covariance matrix :math:`\\mathbf{\\Sigma}`\n",
      "     |  or a positive definite precision matrix :math:`\\mathbf{\\Sigma}^{-1}`\n",
      "     |  or a lower-triangular matrix :math:`\\mathbf{L}` with positive-valued\n",
      "     |  diagonal entries, such that\n",
      "     |  :math:`\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top`. This triangular matrix\n",
      "     |  can be obtained via e.g. Cholesky decomposition of the covariance.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
      "     |      >>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\n",
      "     |      tensor([-0.2102, -0.5429])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor): mean of the distribution\n",
      "     |      covariance_matrix (Tensor): positive-definite covariance matrix\n",
      "     |      precision_matrix (Tensor): positive-definite precision matrix\n",
      "     |      scale_tril (Tensor): lower-triangular factor of covariance, with positive-valued diagonal\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      Only one of :attr:`covariance_matrix` or :attr:`precision_matrix` or\n",
      "     |      :attr:`scale_tril` can be specified.\n",
      "     |  \n",
      "     |      Using :attr:`scale_tril` will be more efficient: all computations internally\n",
      "     |      are based on :attr:`scale_tril`. If :attr:`covariance_matrix` or\n",
      "     |      :attr:`precision_matrix` is passed instead, it is only used to compute\n",
      "     |      the corresponding lower triangular matrices using a Cholesky decomposition.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultivariateNormal\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': Ind...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class NegativeBinomial(torch.distributions.distribution.Distribution)\n",
      "     |  NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Negative Binomial distribution, i.e. distribution\n",
      "     |  of the number of successful independent and identical Bernoulli trials\n",
      "     |  before :attr:`total_count` failures are achieved. The probability\n",
      "     |  of success of each Bernoulli trial is :attr:`probs`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (float or Tensor): non-negative number of negative Bernoulli\n",
      "     |          trials to stop, although the distribution is still valid for real\n",
      "     |          valued count\n",
      "     |      probs (Tensor): Event probabilities of success in the half open interval [0, 1)\n",
      "     |      logits (Tensor): Event log-odds for probabilities of success\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NegativeBinomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_b...\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Normal(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Normal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a normal (also called Gaussian) distribution parameterized by\n",
      "     |  :attr:`loc` and :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # normally distributed with loc=0 and scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the distribution (often referred to as mu)\n",
      "     |      scale (float or Tensor): standard deviation of the distribution\n",
      "     |          (often referred to as sigma)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Normal\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class OneHotCategorical(torch.distributions.distribution.Distribution)\n",
      "     |  OneHotCategorical(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a one-hot categorical distribution parameterized by :attr:`probs` or\n",
      "     |  :attr:`logits`.\n",
      "     |  \n",
      "     |  Samples are one-hot coded vectors of size ``probs.size(-1)``.\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  See also: :func:`torch.distributions.Categorical` for specifications of\n",
      "     |  :attr:`probs` and :attr:`logits`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
      "     |      >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor([ 0.,  0.,  0.,  1.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotCategorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = OneHot()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class OneHotCategoricalStraightThrough(OneHotCategorical)\n",
      "     |  OneHotCategoricalStraightThrough(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a reparameterizable :class:`OneHotCategorical` distribution based on the straight-\n",
      "     |  through gradient estimator from [1].\n",
      "     |  \n",
      "     |  [1] Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\n",
      "     |  (Bengio et al., 2013)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotCategoricalStraightThrough\n",
      "     |      OneHotCategorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = OneHot()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class Pareto(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Pareto(scale, alpha, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Pareto Type 1 distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\n",
      "     |      tensor([ 1.5623])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): Scale parameter of the distribution\n",
      "     |      alpha (float or Tensor): Shape parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Pareto\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, alpha, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': Gre...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Poisson(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Poisson(rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Poisson distribution parameterized by :attr:`rate`, the rate parameter.\n",
      "     |  \n",
      "     |  Samples are nonnegative integers, with a pmf given by\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |    \\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +SKIP(\"poisson_cpu not implemented for 'Long'\")\n",
      "     |      >>> m = Poisson(torch.tensor([4]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 3.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      rate (Number, Tensor): the rate parameter\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Poisson\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'rate': GreaterThanEq(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.exp_family.ExponentialFamily:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class PositiveDefiniteTransform(Transform)\n",
      "     |  PositiveDefiniteTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained matrices to positive-definite matrices.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PositiveDefiniteTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = PositiveDefinite()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 2)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class PowerTransform(Transform)\n",
      "     |  PowerTransform(exponent, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = x^{\\text{exponent}}`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PowerTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, exponent, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  sign\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class RelaxedBernoulli(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a RelaxedBernoulli distribution, parametrized by\n",
      "     |  :attr:`temperature`, and either :attr:`probs` or :attr:`logits`\n",
      "     |  (but not both). This is a relaxed version of the `Bernoulli` distribution,\n",
      "     |  so the values are in (0, 1), and has reparametrizable samples.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n",
      "     |      ...                      torch.tensor([0.1, 0.2, 0.3, 0.99]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      temperature (Tensor): relaxation temperature\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RelaxedBernoulli\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, temperature, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  temperature\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class RelaxedOneHotCategorical(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a RelaxedOneHotCategorical distribution parametrized by\n",
      "     |  :attr:`temperature`, and either :attr:`probs` or :attr:`logits`.\n",
      "     |  This is a relaxed version of the :class:`OneHotCategorical` distribution, so\n",
      "     |  its samples are on simplex, and are reparametrizable.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n",
      "     |      ...                              torch.tensor([0.1, 0.2, 0.3, 0.4]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      temperature (Tensor): relaxation temperature\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): unnormalized log probability for each event\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RelaxedOneHotCategorical\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, temperature, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  temperature\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ReshapeTransform(Transform)\n",
      "     |  ReshapeTransform(in_shape, out_shape, cache_size=0)\n",
      "     |  \n",
      "     |  Unit Jacobian transform to reshape the rightmost part of a tensor.\n",
      "     |  \n",
      "     |  Note that ``in_shape`` and ``out_shape`` must have the same number of\n",
      "     |  elements, just as for :meth:`torch.Tensor.reshape`.\n",
      "     |  \n",
      "     |  Arguments:\n",
      "     |      in_shape (torch.Size): The input event shape.\n",
      "     |      out_shape (torch.Size): The output event shape.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ReshapeTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, in_shape, out_shape, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class SigmoidTransform(Transform)\n",
      "     |  SigmoidTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\frac{1}{1 + \\exp(-x)}` and :math:`x = \\text{logit}(y)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SigmoidTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class SoftmaxTransform(Transform)\n",
      "     |  SoftmaxTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained space to the simplex via :math:`y = \\exp(x)` then\n",
      "     |  normalizing.\n",
      "     |  \n",
      "     |  This is not bijective and cannot be used for HMC. However this acts mostly\n",
      "     |  coordinate-wise (except for the final normalization), and thus is\n",
      "     |  appropriate for coordinate-wise optimization algorithms.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SoftmaxTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = Simplex()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class SoftplusTransform(Transform)\n",
      "     |  SoftplusTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`\\text{Softplus}(x) = \\log(1 + \\exp(x))`.\n",
      "     |  The implementation reverts to the linear function when :math:`x > 20`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SoftplusTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class StackTransform(Transform)\n",
      "     |  StackTransform(tseq, dim=0, cache_size=0)\n",
      "     |  \n",
      "     |  Transform functor that applies a sequence of transforms `tseq`\n",
      "     |  component-wise to each submatrix at `dim`\n",
      "     |  in a way compatible with :func:`torch.stack`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |     x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\n",
      "     |     t = StackTransform([ExpTransform(), identity_transform], dim=1)\n",
      "     |     y = t(x)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tseq, dim=0, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'transforms': typing.List[torch.distributions.trans...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class StickBreakingTransform(Transform)\n",
      "     |  StickBreakingTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained space to the simplex of one additional\n",
      "     |  dimension via a stick-breaking process.\n",
      "     |  \n",
      "     |  This transform arises as an iterated sigmoid transform in a stick-breaking\n",
      "     |  construction of the `Dirichlet` distribution: the first logit is\n",
      "     |  transformed via sigmoid to the first probability and the probability of\n",
      "     |  everything else, and then the process recurses.\n",
      "     |  \n",
      "     |  This is bijective and appropriate for use in HMC; however it mixes\n",
      "     |  coordinates together and is less appropriate for optimization.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StickBreakingTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Simplex()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class StudentT(torch.distributions.distribution.Distribution)\n",
      "     |  StudentT(df, loc=0.0, scale=1.0, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Student's t-distribution parameterized by degree of\n",
      "     |  freedom :attr:`df`, mean :attr:`loc` and scale :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = StudentT(torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # Student's t-distributed with degrees of freedom=2\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df (float or Tensor): degrees of freedom\n",
      "     |      loc (float or Tensor): mean of the distribution\n",
      "     |      scale (float or Tensor): scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StudentT\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df, loc=0.0, scale=1.0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), ...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class TanhTransform(Transform)\n",
      "     |  TanhTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\tanh(x)`.\n",
      "     |  \n",
      "     |  It is equivalent to\n",
      "     |  ```\n",
      "     |  ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n",
      "     |  ```\n",
      "     |  However this might not be numerically stable, thus it is recommended to use `TanhTransform`\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Note that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TanhTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=-1.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class Transform(builtins.object)\n",
      "     |  Transform(cache_size=0)\n",
      "     |  \n",
      "     |  Abstract class for invertable transformations with computable log\n",
      "     |  det jacobians. They are primarily used in\n",
      "     |  :class:`torch.distributions.TransformedDistribution`.\n",
      "     |  \n",
      "     |  Caching is useful for transforms whose inverses are either expensive or\n",
      "     |  numerically unstable. Note that care must be taken with memoized values\n",
      "     |  since the autograd graph may be reversed. For example while the following\n",
      "     |  works with or without caching::\n",
      "     |  \n",
      "     |      y = t(x)\n",
      "     |      t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n",
      "     |  \n",
      "     |  However the following will error when caching due to dependency reversal::\n",
      "     |  \n",
      "     |      y = t(x)\n",
      "     |      z = t.inv(y)\n",
      "     |      grad(z.sum(), [y])  # error because z is x\n",
      "     |  \n",
      "     |  Derived classes should implement one or both of :meth:`_call` or\n",
      "     |  :meth:`_inverse`. Derived classes that set `bijective=True` should also\n",
      "     |  implement :meth:`log_abs_det_jacobian`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      cache_size (int): Size of cache. If zero, no caching is done. If one,\n",
      "     |          the latest single value is cached. Only 0 and 1 are supported.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      domain (:class:`~torch.distributions.constraints.Constraint`):\n",
      "     |          The constraint representing valid inputs to this transform.\n",
      "     |      codomain (:class:`~torch.distributions.constraints.Constraint`):\n",
      "     |          The constraint representing valid outputs to this transform\n",
      "     |          which are inputs to the inverse transform.\n",
      "     |      bijective (bool): Whether this transform is bijective. A transform\n",
      "     |          ``t`` is bijective iff ``t.inv(t(x)) == x`` and\n",
      "     |          ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in\n",
      "     |          the codomain. Transforms that are not bijective should at least\n",
      "     |          maintain the weaker pseudoinverse properties\n",
      "     |          ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.\n",
      "     |      sign (int or Tensor): For bijective univariate transforms, this\n",
      "     |          should be +1 or -1 depending on whether transform is monotone\n",
      "     |          increasing or decreasing.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class TransformedDistribution(torch.distributions.distribution.Distribution)\n",
      "     |  TransformedDistribution(base_distribution, transforms, validate_args=None)\n",
      "     |  \n",
      "     |  Extension of the Distribution class, which applies a sequence of Transforms\n",
      "     |  to a base distribution.  Let f be the composition of transforms applied::\n",
      "     |  \n",
      "     |      X ~ BaseDistribution\n",
      "     |      Y = f(X) ~ TransformedDistribution(BaseDistribution, f)\n",
      "     |      log p(Y) = log p(X) + log |det (dX/dY)|\n",
      "     |  \n",
      "     |  Note that the ``.event_shape`` of a :class:`TransformedDistribution` is the\n",
      "     |  maximum shape of its base distribution and its transforms, since transforms\n",
      "     |  can introduce correlations among events.\n",
      "     |  \n",
      "     |  An example for the usage of :class:`TransformedDistribution` would be::\n",
      "     |  \n",
      "     |      # Building a Logistic Distribution\n",
      "     |      # X ~ Uniform(0, 1)\n",
      "     |      # f = a + b * logit(X)\n",
      "     |      # Y ~ f(X) ~ Logistic(a, b)\n",
      "     |      base_distribution = Uniform(0, 1)\n",
      "     |      transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\n",
      "     |      logistic = TransformedDistribution(base_distribution, transforms)\n",
      "     |  \n",
      "     |  For more examples, please look at the implementations of\n",
      "     |  :class:`~torch.distributions.gumbel.Gumbel`,\n",
      "     |  :class:`~torch.distributions.half_cauchy.HalfCauchy`,\n",
      "     |  :class:`~torch.distributions.half_normal.HalfNormal`,\n",
      "     |  :class:`~torch.distributions.log_normal.LogNormal`,\n",
      "     |  :class:`~torch.distributions.pareto.Pareto`,\n",
      "     |  :class:`~torch.distributions.weibull.Weibull`,\n",
      "     |  :class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli` and\n",
      "     |  :class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_distribution, transforms, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Uniform(torch.distributions.distribution.Distribution)\n",
      "     |  Uniform(low, high, validate_args=None)\n",
      "     |  \n",
      "     |  Generates uniformly distributed random samples from the half-open interval\n",
      "     |  ``[low, high)``.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n",
      "     |      >>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\n",
      "     |      >>> # xdoctest: +SKIP\n",
      "     |      tensor([ 2.3418])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      low (float or Tensor): lower range (inclusive).\n",
      "     |      high (float or Tensor): upper range (exclusive).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Uniform\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, low, high, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'high': Dependent(), 'low': Dependent()}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class VonMises(torch.distributions.distribution.Distribution)\n",
      "     |  VonMises(loc, concentration, validate_args=None)\n",
      "     |  \n",
      "     |  A circular von Mises distribution.\n",
      "     |  \n",
      "     |  This implementation uses polar coordinates. The ``loc`` and ``value`` args\n",
      "     |  can be any real number (to facilitate unconstrained optimization), but are\n",
      "     |  interpreted as angles modulo 2 pi.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # von Mises distributed with loc=1 and concentration=1\n",
      "     |      tensor([1.9777])\n",
      "     |  \n",
      "     |  :param torch.Tensor loc: an angle in radians.\n",
      "     |  :param torch.Tensor concentration: concentration parameter\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VonMises\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      The sampling algorithm for the von Mises distribution is based on the\n",
      "     |      following paper: D.J. Best and N.I. Fisher, \"Efficient simulation of the\n",
      "     |      von Mises distribution.\" Applied Statistics (1979): 152-157.\n",
      "     |      \n",
      "     |      Sampling is always done in double precision internally to avoid a hang\n",
      "     |      in _rejection_sample() for small values of the concentration, which\n",
      "     |      starts to happen for single precision around 1e-4 (see issue #88443).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      The provided mean is the circular one.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      The provided variance is the circular one.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc...\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self) -> torch.Tensor\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Weibull(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Weibull(scale, concentration, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a two-parameter Weibull distribution.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |      >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\n",
      "     |      tensor([ 0.4784])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): Scale parameter of distribution (lambda).\n",
      "     |      concentration (float or Tensor): Concentration parameter of distribution (k/shape).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Weibull\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'sca...\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Wishart(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Wishart(df: Union[torch.Tensor, numbers.Number], covariance_matrix: Optional[torch.Tensor] = None, precision_matrix: Optional[torch.Tensor] = None, scale_tril: Optional[torch.Tensor] = None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Wishart distribution parameterized by a symmetric positive definite matrix :math:`\\Sigma`,\n",
      "     |  or its Cholesky decomposition :math:`\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top`\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # xdoctest: +SKIP(\"FIXME: scale_tril must be at least two-dimensional\")\n",
      "     |      >>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))\n",
      "     |      >>> m.sample()  # Wishart distributed with mean=`df * I` and\n",
      "     |      >>>             # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df (float or Tensor): real-valued parameter larger than the (dimension of Square matrix) - 1\n",
      "     |      covariance_matrix (Tensor): positive-definite covariance matrix\n",
      "     |      precision_matrix (Tensor): positive-definite precision matrix\n",
      "     |      scale_tril (Tensor): lower-triangular factor of covariance, with positive-valued diagonal\n",
      "     |  Note:\n",
      "     |      Only one of :attr:`covariance_matrix` or :attr:`precision_matrix` or\n",
      "     |      :attr:`scale_tril` can be specified.\n",
      "     |      Using :attr:`scale_tril` will be more efficient: all computations internally\n",
      "     |      are based on :attr:`scale_tril`. If :attr:`covariance_matrix` or\n",
      "     |      :attr:`precision_matrix` is passed instead, it is only used to compute\n",
      "     |      the corresponding lower triangular matrices using a Cholesky decomposition.\n",
      "     |      'torch.distributions.LKJCholesky' is a restricted Wishart distribution.[1]\n",
      "     |  \n",
      "     |  **References**\n",
      "     |  \n",
      "     |  [1] Wang, Z., Wu, Y. and Chu, H., 2018. `On equivalence of the LKJ distribution and the restricted Wishart distribution`.\n",
      "     |  [2] Sawyer, S., 2007. `Wishart Distributions and Inverse-Wishart Sampling`.\n",
      "     |  [3] Anderson, T. W., 2003. `An Introduction to Multivariate Statistical Analysis (3rd ed.)`.\n",
      "     |  [4] Odell, P. L. & Feiveson, A. H., 1966. `A Numerical Procedure to Generate a SampleCovariance Matrix`. JASA, 61(313):199-203.\n",
      "     |  [5] Ku, Y.-C. & Bloomfield, P., 2010. `Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Wishart\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: Union[torch.Tensor, numbers.Number], covariance_matrix: Optional[torch.Tensor] = None, precision_matrix: Optional[torch.Tensor] = None, scale_tril: Optional[torch.Tensor] = None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([]), max_try_correction=None) -> torch.Tensor\n",
      "     |      .. warning::\n",
      "     |          In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples.\n",
      "     |          Several tries to correct singular samples are performed by default, but it may end up returning\n",
      "     |          singular matrix samples. Singular samples may return `-inf` values in `.log_prob()`.\n",
      "     |          In those cases, the user should validate the samples and either fix the value of `df`\n",
      "     |          or adjust `max_try_correction` value for argument in `.rsample` accordingly.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  arg_constraints = {'covariance_matrix': PositiveDefinite(), 'df': Grea...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = PositiveDefinite()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand: bool = True) -> torch.Tensor\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value: torch.Tensor) -> torch.Tensor\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self) -> torch.Tensor\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape: Union[torch.Size, List[int], Tuple[int, ...]] = torch.Size([])) -> torch.Tensor\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n: int) -> torch.Tensor\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value: bool) -> None\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "\n",
      "FUNCTIONS\n",
      "    kl_divergence(p: torch.distributions.distribution.Distribution, q: torch.distributions.distribution.Distribution) -> torch.Tensor\n",
      "        Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            KL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx\n",
      "        \n",
      "        Args:\n",
      "            p (Distribution): A :class:`~torch.distributions.Distribution` object.\n",
      "            q (Distribution): A :class:`~torch.distributions.Distribution` object.\n",
      "        \n",
      "        Returns:\n",
      "            Tensor: A batch of KL divergences of shape `batch_shape`.\n",
      "        \n",
      "        Raises:\n",
      "            NotImplementedError: If the distribution types have not been registered via\n",
      "                :meth:`register_kl`.\n",
      "        KL divergence is currently implemented for the following distribution pairs:\n",
      "            * :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Bernoulli`\n",
      "            * :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Poisson`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Binomial` and :class:`~torch.distributions.Binomial`\n",
      "            * :class:`~torch.distributions.Categorical` and :class:`~torch.distributions.Categorical`\n",
      "            * :class:`~torch.distributions.Cauchy` and :class:`~torch.distributions.Cauchy`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Dirichlet` and :class:`~torch.distributions.Dirichlet`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.ExponentialFamily` and :class:`~torch.distributions.ExponentialFamily`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Geometric` and :class:`~torch.distributions.Geometric`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.HalfNormal` and :class:`~torch.distributions.HalfNormal`\n",
      "            * :class:`~torch.distributions.Independent` and :class:`~torch.distributions.Independent`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Laplace`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n",
      "            * :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n",
      "            * :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n",
      "            * :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Laplace`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.OneHotCategorical` and :class:`~torch.distributions.OneHotCategorical`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Bernoulli`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Binomial`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Poisson`\n",
      "            * :class:`~torch.distributions.TransformedDistribution` and :class:`~torch.distributions.TransformedDistribution`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Uniform`\n",
      "    \n",
      "    register_kl(type_p, type_q)\n",
      "        Decorator to register a pairwise function with :meth:`kl_divergence`.\n",
      "        Usage::\n",
      "        \n",
      "            @register_kl(Normal, Normal)\n",
      "            def kl_normal_normal(p, q):\n",
      "                # insert implementation here\n",
      "        \n",
      "        Lookup returns the most specific (type,type) match ordered by subclass. If\n",
      "        the match is ambiguous, a `RuntimeWarning` is raised. For example to\n",
      "        resolve the ambiguous situation::\n",
      "        \n",
      "            @register_kl(BaseP, DerivedQ)\n",
      "            def kl_version1(p, q): ...\n",
      "            @register_kl(DerivedP, BaseQ)\n",
      "            def kl_version2(p, q): ...\n",
      "        \n",
      "        you should register a third most-specific implementation, e.g.::\n",
      "        \n",
      "            register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n",
      "        \n",
      "        Args:\n",
      "            type_p (type): A subclass of :class:`~torch.distributions.Distribution`.\n",
      "            type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Bernoulli', 'Beta', 'Binomial', 'Categorical', 'Cauchy', '...\n",
      "    biject_to = <torch.distributions.constraint_registry.ConstraintRegistr...\n",
      "    identity_transform = ComposeTransform(\n",
      "        \n",
      "    )\n",
      "    transform_to = <torch.distributions.constraint_registry.ConstraintRegi...\n",
      "\n",
      "FILE\n",
      "    /home/cqilab/anaconda3/envs/anas_env/lib/python3.11/site-packages/torch/distributions/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.distributions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
